### **Гомоскедастичность остатков в регрессии

**Гомоскедастичность** (от греч. *homos* — одинаковый, *skedastos* — рассеянный) — это свойство регрессионной модели, при котором **дисперсия остатков** (ошибок) **постоянна** для всех значений:
- Независимых переменных (x1, x2, ...),
- Предсказанных значений (\(\hat{y}\)).

**Формальное определение**:  
$$
\text{Var}(\epsilon_i) = \sigma^2 \quad \text{для всех } i=1, \dots, n.
$$

---

#### **Почему это важно?**
Гомоскедастичность — ключевое условие для:
1. **Эффективности МНК-оценок(Метод наименьших квадратов)**:  
   - Оценки коэффициентов имеют **наименьшую дисперсию** (теорема Гаусса-Маркова).  
2. **Корректности статистических тестов**:  
   - t-тесты, F-тесты и доверительные интервалы работают корректно.  
3. **Интерпретируемости модели**:  
   - Ошибки предсказаний одинаково надежны для всех диапазонов данных.

---

#### **Визуализация и примеры**

##### **Идеальный случай гомоскедастичности**
**Генерация данных**:  
$$
y = 2x + 1 + \epsilon, \quad \epsilon \sim N(0, 1).
$$  
**График остатков vs предсказанные значения (\(\hat{y}\))**:  
```plaintext
  |         *
  |       *   *  
  |     *     *  
  |   *  *  *  
  |_*_________\(\hat{y}\)
```
**Характеристики**:  
- Остатки равномерно рассеяны вокруг нуля.  
- Ширина "облака" остатков не меняется.

##### **Сравнение с гетероскедастичностью**
**Генерация данных с растущей дисперсией**:  
$$
y = 2x + 1 + \epsilon, \quad \epsilon \sim N(0, x^2).
$$  
**График остатков**:  
```plaintext
  |        *  
  |      *   *  
  |    * *     * *  
  |  *      *     *  
  |__________\(\hat{y}\)
```
**Различия**:  
- У гомоскедастичных данных нет "воронки" или "веера".  
- Дисперсия не зависит от \(x\) или \(\hat{y}\).

---

#### **Математическая проверка**
**Дисперсия остатков** должна быть постоянной:  
$$
\frac{1}{n} \sum_{i=1}^n (e_i - \bar{e})^2 \approx \text{const}, \quad \text{где } e_i = y_i - \hat{y}_i.
$$


В данной формуле **\(\bar{e}\)** — это **среднее значение остатков** (ошибок) модели, вычисляемое как:

$$
\bar{e} = \frac{1}{n} \sum_{i=1}^n e_i,
$$

где:
- $$ e_i = y_i - \hat{y}_i $$ — остаток для \(i\)-го наблюдения,
- \(n\) — количество наблюдений.

---



Для **линейной регрессии с константой (свободным членом \( b_0 \))** сумма остатков **строго равна нулю**:
$$
\sum_{i=1}^n e_i = 0,
$$
где $$ e_i = y_i - \hat{y}_i $$ — остатки модели. Это не просто вероятностное свойство, а **строгое математическое следствие метода наименьших квадратов (МНК)**.

---

### **Почему так происходит?**
1. **Условие оптимизации МНК**:  
   При подборе коэффициентов регрессии (включая \( b_0 \)) МНК минимизирует сумму квадратов остатков:
   $$
   S(b_0, b_1, \dots, b_p) = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \to \min.
   $$
2. **Производная по \( b_0 \)** (свободному члену):  
   Чтобы найти минимум, берём частную производную $$ \frac{\partial S}{\partial b_0} = -2 \sum_{i=1}^n (y_i - b_0 - b_1 x_{i1} - \dots - b_p x_{ip}) = 0.
   $$
   Упрощаем:
   $$
   \sum_{i=1}^n e_i = 0 \quad \text{(так как } e_i = y_i - \hat{y}_i = y_i - (b_0 + b_1 x_{i1} + \dots + b_p x_{ip})).
   $$

---

### **Когда это не выполняется?**
1. **Если в модели нет константы \( b_0 \)**.  
   Например, в регрессии \( y = b_1 x \) сумма остатков может быть ненулевой.  
   *Почему?* Потому что производная \( \frac{\partial S}{\partial b_0} \) не учитывается в оптимизации.

2. **При ошибках в данных или вычислениях**:  
   - Пропущенные значения,  
   - Численная нестабильность алгоритма.

1. **Геометрическая интерпретация**:  
   - Остатки \(e_i\) — это проекции ошибок на пространство, ортогональное предикторам.  
   - Вектор остатков **ортогонален** вектору из единиц (столбец констант в матрице \(X\)), поэтому их скалярное произведение равно нулю:  
     $$
     \mathbf{1}^T \mathbf{e} = \sum_{i=1}^n e_i = 0.
     $$

---

### **2. Почему \(\sum e_i = 0\) не зависит от других уравнений?**
- **Уравнение для \(b_0\)** содержит **только сумму остатков** \(\sum e_i\), так как константа \(b_0\) умножается на столбец из единиц в матрице данных \(X\).
- **Уравнения для \(b_k\)** включают предикторы \(x_k\), но **не меняют условие \(\sum e_i = 0\)**, потому что:
  - Они отвечают за "наклон" регрессии (влияние \(x_k\) на \(y\)),
  - А \(b_0\) компенсирует "сдвиг" модели, чтобы остатки были сбалансированы.

**Аналогия**:  
Представьте, что вы подбираете линейную функцию $$ y = b_0 + b_1 x $$ к данным.  
- \(b0\) (константа) регулирует **высоту** линии, чтобы "поднять" или "опустить" её так, чтобы положительные и отрицательные остатки взаимно уничтожились (\(\sum e_i = 0\)).
- \(b1\) (наклон) регулирует **угол** линии, минимизируя сумму квадратов остатков, но не влияет на баланс их сумм.

---

### **4. Геометрическая интерпретация**
- **Остатки \(e\)** — это вектор длины \(n\) (по числу наблюдений).  
- Условие \(\sum e_i = 0\) означает, что \(e\) ортогонален вектору из единиц \(\mathbf{1}\) (столбец констант в \(X\)).  
- Условия \(\sum x_k e_i = 0\) означают, что \(e\) ортогонален всем предикторам \(x_k\).  
- Таким образом, МНК находит такое решение, где остатки **перпендикулярны всем столбцам матрицы \(X\)** (включая константу).

---

### **5. Что, если константы \(b_0\) нет?**
Если модель имеет вид \(y = b_1 x_1 + \dots + b_p x_p\) (без \(b_0\)), то:  
- Уравнение \(\sum e_i = 0\) **не выполняется**, потому что в матрице \(X\) нет столбца из единиц.  
- Сумма остатков может быть ненулевой, и это ухудшает свойства модели (например, смещает прогнозы).

---
### **1. Идеальный случай vs Реальность**
- **Идеальный случай**:  
  Если данные **точно** соответствуют линейной модели \( y = b_0 + b_1 x \), то МНК найдёт коэффициенты, где все остатки \( e_i = 0 \), и сумма остатков тоже будет нулевой.  
  *Пример*: Точки на прямой \( y = 2x + 1 \).

- **Реальные данные**:  
  Наблюдения включают **шум** (случайные ошибки) или **выбросы**. Модель \( y = b_0 + b_1 x \) уже не может пройти через все точки, но МНК подбирает коэффициенты так, чтобы:
  - Сумма остатков **равнялась нулю** (есть константа \( b_0 \)),
  - Сумма квадратов остатков **была минимальна**.

---

### **2. Почему сумма остатков равна нулю даже при шуме?**
#### **Роль константы \( b_0 \)**
- \( b_0 \) "поднимает" или "опускает" линию регрессии так, чтобы положительные и отрицательные отклонения от неё **уравновесили друг друга**.  
- Это гарантирует, что **средняя ошибка** равна нулю, даже если отдельные \( e_i \neq 0 \).

#### **Пример с шумом**
Допустим, истинная модель:  
\[
y = 2x + 1 + \epsilon, \quad \epsilon \sim N(0, \sigma^2).
\]  
Наблюдаемые данные:  
| \( x \) | \( y \) (с шумом) |  
|---------|--------------------|  
| 1       | 3.1                |  
| 2       | 4.9                |  
| 3       | 7.2                |  

МНК подберёт коэффициенты \( \hat{y} = b_0 + b_1 x \), где:
- \( b_1 \) отвечает за наклон (связь \( x \) и \( y \)),
- \( b_0 \) корректирует высоту так, чтобы \( \sum e_i = 0 \).

**Остатки**:  
\[
e = [3.1 - \hat{y}(1), 4.9 - \hat{y}(2), 7.2 - \hat{y}(3)].
\]  
Условие \( \sum e_i = 0 \) выполняется, даже если отдельные \( e_i \neq 0 \).

---

### **3. Как модель работает с выбросами?**
- **Выбросы** — это точки, сильно отклоняющиеся от общей закономерности.  
- МНК **чувствителен к выбросам**, потому что минимизирует **сумму квадратов** остатков (большие ошибки вносят огромный вклад в функцию потерь).  

**Пример**:  
Добавим выброс в данные: \( (x, y) = (4, 20) \).  
- Линия регрессии сместится в сторону выброса.  
- Но сумма остатков **останется нулевой** (за счёт \( b_0 \)), хотя модель станет менее точной.

**Как бороться**:  
- Использовать **робастную регрессию** (например, метод Хубера), которая менее чувствительна к выбросам.  
- Удалять или корректировать аномальные точки.

---

### **4. Случай нелинейной зависимости**
Если истинная зависимость **нелинейна** (например, \( y = e^x \)), линейная модель даст плохую аппроксимацию:
- Сумма остатков всё равно будет \( \sum e_i = 0 \) (из-за \( b_0 \)),  
- Но остатки **систематически отклоняются** от нуля (например, сначала отрицательные, потом положительные).  

**Решение**:  
- Преобразовать данные (например, взять логарифм \( y \)).  
- Использовать **нелинейную регрессию** (полиномиальную, сплайны).

---

### **5. Практический пример в Python**
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Данные с шумом
np.random.seed(42)
x = np.array([1, 2, 3, 4])
y_true = 2 * x + 1
y_noisy = y_true + np.random.normal(0, 0.5, size=len(x))  # Добавляем шум

# Модель
model = LinearRegression().fit(x.reshape(-1, 1), y_noisy)
y_pred = model.predict(x.reshape(-1, 1))
residuals = y_noisy - y_pred

# Проверка суммы остатков
print("Сумма остатков:", np.sum(residuals))  # ~0 (-1.11e-16)

# График
plt.scatter(x, y_noisy, label="Данные с шумом")
plt.plot(x, y_pred, color="red", label="Линия регрессии")
plt.legend()
plt.show()
```
**Вывод**:  
```
Сумма остатков: -1.1102230246251565e-16  # Практически 0
```
![](https://i.imgur.com/8W7Rz7L.png)  
Несмотря на шум, сумма остатков нулевая, а линия регрессии отражает общий тренд.

---

### **6. Итог**
- **Сумма остатков \( \sum e_i = 0 \)** — это **свойство МНК-модели с константой**, а не признак идеального описания данных.  
- В реальности данные всегда содержат шум, выбросы или нелинейности, поэтому:  
  - Отдельные остатки \( e_i \) не равны нулю,  
  - Но их среднее значение (\( \frac{1}{n} \sum e_i \)) равно нулю благодаря \( b_0 \).  
- Если модель плохо описывает данные (например, из-за нелинейности), это видно по **систематическим паттернам в остатках**, а не по их сумме.  

**Как проверить адекватность модели?**  
1. График остатков \( e_i \) vs предсказанные значения \( \hat{y}_i \).  
2. Тесты на гетероскедастичность (Бреуша-Пагана).  
3. Q-Q plot остатков для проверки нормальности.  

Таким образом, условие \( \sum e_i = 0 \) — это не "волшебное свойство" идеальной аппроксимации, а **техническое следствие оптимизации МНК**, которое выполняется даже для зашумлённых данных.

---

#### **5. Статистические тесты**
##### **Тест Бреуша-Пагана**
1. **Регрессия остатков** на предикторы:  
   \[
   e_i^2 = \alpha_0 + \alpha_1 x_{i1} + \dots + \alpha_p x_{ip} + u_i.
   \]  
2. **Проверка гипотезы**:  
   - \(H_0\): \(\alpha_1 = \alpha_2 = \dots = \alpha_p = 0\) (гомоскедастичность).  
   - \(H_1\): Хотя бы один \(\alpha_j \neq 0\) (гетероскедастичность).  

**Интерпретация**:  
- Если p-value > 0.05, принимаем \(H_0\).

##### **Тест Уайта**
Более общий: включает квадраты и взаимодействия предикторов.  

**Пример в Python**:  
```python
import statsmodels.stats.diagnostic as dg
test_result = dg.het_breuschpagan(model.resid, model.model.exog)
print(f"LM-statistic: {test_result[0]}, p-value: {test_result[1]}")
```

---

#### **6. Последствия нарушения**
Если гомоскедастичность **есть**, но вы ошибочно решили, что её нет:  
- **Потеря точности**: WLS или робастные ошибки менее эффективны, чем МНК.  
- **Усложнение модели**: Напрасно применяете методы коррекции.

---

#### **7. Практические рекомендации**
1. **Всегда начинайте с графиков остатков**.  
2. **Для малых выборок** полагайтесь на визуальную проверку (тесты могут ошибаться).  
3. **Если гомоскедастичность подтверждена**:  
   - Используйте обычный МНК — это оптимальный метод.  

**Пример кода для визуализации**:  
```python
import matplotlib.pyplot as plt
plt.scatter(model.fittedvalues, model.resid)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Предсказанные значения")
plt.ylabel("Остатки")
plt.title("Проверка гомоскедастичности")
plt.show()
```

---

### **Итог**
**Гомоскедастичность** — это "идеальный" сценарий для регрессии. Её наличие гарантирует:  
- Наилучшие оценки коэффициентов,  
- Корректные p-значения и доверительные интервалы.  

**Что делать, если она есть?**  
- Работать со стандартной МНК-моделью без дополнительных корректировок.  
- Периодически перепроверять данные на новых наблюдениях.  

**Графический пример гомоскедастичности**:  
```plaintext
  |         • •
  |       •   •  
  |     •     •  
  |   •  •  •  
  |_•_________\(\hat{y}\)
```  
(Остатки равномерно распределены в "полосе" постоянной ширины.)