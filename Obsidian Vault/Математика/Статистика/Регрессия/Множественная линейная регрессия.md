**Расширение простой линейной регрессии([[Линейная регрессия]]) на случай нескольких независимых переменных.**  

Если в простой регрессии мы предсказываем `y` по одному признаку `x`:  
$$
y = a \cdot x + b + \epsilon,  
$$  
то в **множественной регрессии** учитывается несколько признаков:  
$$
y = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_p x_p + \epsilon,  
$$  
где:  
- \( y \) — целевая переменная,  
- \( x1, x2, ..., xp \) — независимые переменные (признаки),  
- \( b0 \) — свободный член (интерсепт),  
- \( b1, b2, ..., bp \) — коэффициенты регрессии,  
- \( epsilon \) — случайная ошибка (шум).  

---

## **Метод наименьших квадратов (МНК) для множественной регрессии**  
Цель — минимизировать **сумму квадратов остатков**:  
$$
S(b_0, b_1, \dots, b_p) = \sum_{i=1}^n \left( y_i - (b_0 + b_1 x_{i1} + \dots + b_p x_{ip}) \right)^2 \to \min.  
$$  
### **Подробный вывод коэффициентов множественной линейной регрессии методом наименьших квадратов (МНК)**  

Рассмотрим модель с  p независимыми переменными:  
$$
y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip} + \epsilon_i, \quad i = 1, \dots, n.  
$$  

**Цель**: найти коэффициенты $$ b_0, b_1, \dots, b_p $$, минимизирующие сумму квадратов остатков:  
$$
S(b_0, b_1, \dots, b_p) = \sum_{i=1}^n \left( y_i - (b_0 + b_1 x_{i1} + \dots + b_p x_{ip}) \right)^2 \to \min.  
$$  

Для этого возьмём частные производные S по каждому коэффициенту $$ b_k, ( k = 0, 1, \dots, p ) $$ и приравняем их к нулю.  

## **1. Частные производные и система уравнений**  

### **(1) Производная по \( b0 \):**  
$$
\frac{\partial S}{\partial b_0} = -2 \sum_{i=1}^n \left( y_i - b_0 - b_1 x_{i1} - \dots - b_p x_{ip} \right) = 0.  
$$  
Упрощаем:  
$$
\sum_{i=1}^n y_i = n b_0 + b_1 \sum_{i=1}^n x_{i1} + b_2 \sum_{i=1}^n x_{i2} + \dots + b_p \sum_{i=1}^n x_{ip}.  
$$  
Это **первое уравнение системы**.  

---

### **(2) Производная по bk  ( k = 1, ..., p \)):**  
Для каждого bk:  
$$
\frac{\partial S}{\partial b_k} = -2 \sum_{i=1}^n x_{ik} \left( y_i - b_0 - b_1 x_{i1} - \dots - b_p x_{ip} \right) = 0.  
$$  
Упрощаем:  
$$
\sum_{i=1}^n x_{ik} y_i = b_0 \sum_{i=1}^n x_{ik} + b_1 \sum_{i=1}^n x_{ik} x_{i1} + \dots + b_p \sum_{i=1}^n x_{ik} x_{ip}.  
$$  
Это даёт ещё p уравнений.  

---

## **2. Система нормальных уравнений**  

Итого, получаем систему из \( p+1 \) линейных уравнений:  

1. Для \( b0 \):  
   $$
   n b_0 + b_1 \sum x_{i1} + b_2 \sum x_{i2} + \dots + b_p \sum x_{ip} = \sum y_i.  
   $$  

2. Для \( b1 \):  
   $$
   b_0 \sum x_{i1} + b_1 \sum x_{i1}^2 + b_2 \sum x_{i1} x_{i2} + \dots + b_p \sum x_{i1} x_{ip} = \sum x_{i1} y_i.  
   $$  

3. Для \( b2 \):  
   $$
   b_0 \sum x_{i2} + b_1 \sum x_{i1} x_{i2} + b_2 \sum x_{i2}^2 + \dots + b_p \sum x_{i2} x_{ip} = \sum x_{i2} y_i.  
   $$  

   .
   .
   .

\( p+1 \). Для \( bp \):  
   $$
   b_0 \sum x_{ip} + b_1 \sum x_{i1} x_{ip} + b_2 \sum x_{i2} x_{ip} + \dots + b_p \sum x_{ip}^2 = \sum x_{ip} y_i.  
   $$  

---

## **3. Решение системы**  

Эту систему можно решить **аналитически** (методом Крамера, Гаусса) или **численно** (например, методом LU-разложения).  

### **Пример для случая \( p = 2 \)** (две независимые переменные):  

Модель:  
$$
y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \epsilon_i.  
$$  

Система уравнений:  
$$ n b_0 + b_1 \sum x_{i1} + b_2 \sum x_{i2} = \sum y_i $$
$$ b_0 \sum x_{i1} + b_1 \sum x_{i1}^2 + b_2 \sum x_{i1} x_{i2} = \sum x_{i1} y_i $$  
$$ b_0 \sum x_{i2} + b_1 \sum x_{i1} x_{i2} + b_2 \sum x_{i2}^2 = \sum x_{i2} y_i $$  

Выражаем \( b0 \) из первого уравнения:  
$$
b_0 = \frac{1}{n} \left( \sum y_i - b_1 \sum x_{i1} - b_2 \sum x_{i2} \right).  
$$  

Подставляем \( b0 \) во второе и третье уравнения и решаем систему для \( b1 \) и \( b \).  

---

## **4. Вывод коэффициентов (общий случай)**  

В общем виде коэффициенты можно выразить через **ковариации**([[Ковариация]]) и **дисперсии**([[Дисперсия]]):  

- \( b1 \) зависит от корреляции \( x1 \) и \( y \), скорректированной на влияние других переменных,  
- \( b2 \) аналогично зависит от корреляции \( x2 \) и \( y \) и т.д.  

Финальные формулы громоздки, поэтому на практике используют **матричный метод** 

### **Матричная форма решения**  
Удобно записать в матрицах:  
$$
\mathbf{y} = \mathbf{X} \mathbf{b} + \mathbf{\epsilon},  
$$  
где:  
- y — вектор значений целевой переменной (\( n \times 1 \)),  
- X — матрица признаков $$( n \times (p+1) $$, с колонкой из единиц для \( b0 \)),  
- b — вектор коэффициентов $$( (p+1) \times 1) $$,  
- epsilon — вектор ошибок.  

**Решение МНК:**  
$$
\mathbf{b} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.  
$$  

*(Если матрица $$ \mathbf{X}^T \mathbf{X} $$ вырождена, используют регуляризацию или псевдообратную матрицу.)*  

---

## **3. Интерпретация коэффициентов**  
- **\( bj \)** показывает, насколько изменится \( y \) при увеличении \( xj \) на **1 единицу**, **при условии, что все остальные переменные фиксированы**.  
  - Пример: Если  b1 = 2.5, то рост x1 на 1 увеличит y на 2.5 (если x2, x3, ... не меняются).  

- **Важно:** Коэффициенты зависят от масштаба данных! Если \( x1 \) в километрах, а \( x2 \) в сантиметрах, их веса несравнимы → нужно масштабирование.  

---

## **4. Оценка качества модели**  
### **Основные метрики:**  
1. **[[Коэффициент детерминации ( R^2 )]]:**  
   - Доля объяснённой дисперсии: $$ R^2 = 1 - \frac{\text{SS}_{\text{res}}}{\text{SS}_{\text{tot}}} $$
   - **Проблема:** \( R^2 \) всегда растёт при добавлении новых признаков → используют **скорректированный \( R^2 \)**.  

2. **F-статистика:**  
   - Проверяет значимость модели в целом (гипотеза \( H_0 \): все \( b_j = 0 \), кроме \( b_0 \)).  

3. **p-значения для коэффициентов:**  
   - Если \( p < 0.05 \), коэффициент считается статистически значимым.  

4. **AIC / BIC:**  
   - Критерии для выбора модели с учётом числа параметров (штрафуют за переобучение).  

---
### **Ограничения множественной линейной регрессии**  
Ключевое отличие от простой регрессии — использование **нескольких независимых переменных** (\(x1, x2, ..., xp\)). Однако это накладывает дополнительные требования на данные и модель. Вот основные ограничения:

## **Линейность связи**  
**Ограничение**: Зависимость между **каждым предиктором** \(xj\) и целевой переменной \(y\) должна быть **линейной**.  

**Проверка**:  
- Графики остатков (residual plots) — если есть нелинейные паттерны, модель неадекватна.  
- Добавление квадратичных или взаимодействующих членов может помочь.  

**Пример нарушения**:  
Если истинная зависимость $$ y = x_1^2 + x_2 $$, линейная модель $$ y = b_0 + b_1 x_1 + b_2 x_2 $$ даст плохие предсказания.

---

## **Отсутствие мультиколлинеарности**  
**Ограничение**: Независимые переменные **не должны быть сильно коррелированы** друг с другом(x1, x2, ... - не должны сильно коррелировать друг с другом).  

**Проблемы**:  
- Коэффициенты становятся **неустойчивыми** (малые изменения данных приводят к большим изменениям bj.  
- Их **интерпретация** теряет смысл (невозможно выделить вклад отдельного предиктора).  

**Диагностика**:  
- **Коэффициент VIF (Variance Inflation Factor)**:  
  - VIF > 5 — умеренная корреляция,  
  - VIF > 10 — критическая (нужно удалять предикторы или применять регуляризацию).  
- **Матрица корреляций**: Если \(|r(x_j, x_k)| > 0.8\), это тревожный сигнал.  

**Решение**:  
- Удаление одного из коррелирующих предикторов.  
- Использование **PCA** или **гребневой регрессии (Ridge)**.

---

## **3. Гомоскедастичность остатков**  
**Ограничение**: Дисперсия ошибок (epsilon_i) должна быть **постоянной** для всех значений (xj).  

**Нарушение (гетероскедастичность)**:  
- Остатки "воронкой" или "веером" на графике (например, ошибки растут с ростом \(y\)).  
- Снижает **эффективность МНК-оценок**.  

**Решение**:  
- Преобразование \(y\) (например, логарифмирование).  
- Использование **взвешенного МНК (WLS)**.  

---

## **4. Нормальность остатков**  
**Ограничение**: Для малых выборок (\(n < 30\)) остатки должны быть **нормально распределены**.  

**Проверка**:  
- Гистограмма остатков,  
- Q-Q plot,  
- Тест Шапиро-Уилка.  

**Что делать при нарушении**:  
- Увеличить выборку,  
- Применить **непараметрические методы** (например, бутстрап).  

---

## **5. Отсутствие автокорреляции**  
**Ограничение**: Ошибки \(\epsilon_i\) **не должны коррелировать** друг с другом (актуально для временных рядов).  

**Диагностика**:  
- Тест Дарбина-Уотсона:  
  - DW ≈ 2 — нет автокорреляции,  
  - DW < 1 или > 3 — проблема.  

**Решение**:  
- Модели **ARIMA**,  
- Добавление лаговых переменных.  

---

## **6. Отсутствие выбросов и влиятельных наблюдений**  
**Ограничение**: Данные не должны содержать **аномальных точек**, сильно искажающих коэффициенты.  

**Проверка**:  
- Графики **Cook’s distance**,  
- Анализ стандартизированных остатков (если \(|res| > 3\), это выброс).  

**Решение**:  
- Удаление выбросов,  
- Использование **робастной регрессии** (Theil-Sen, Huber).  

---

## **7. Размер выборки**  
**Ограничение**: Число наблюдений \(n\) должно быть **достаточным** для числа предикторов \(p\).  

**Правило**:  
- Минимум \(n > 10 \cdot p\) (для устойчивых оценок),  
- Идеально \(n > 30 \cdot p\).  

**При малой выборке**:  
- Высокий риск **переобучения**,  
- Используйте регуляризацию (Lasso/Ridge).  

---

## **8. Отсутствие пропущенных значений**  
**Проблема**: Пропуски в данных (\(NaN\)) нарушают расчёты.  

**Решение**:  
- Удаление строк с пропусками,  
- Импутация (средним, медианой, регрессией).  









---
## **5. Предположения модели**  
Для корректности выводов должны выполняться:  
1. **Линейность** связи между \( X \) и \( y \).  
2. **Отсутствие мультиколлинеарности** (сильной корреляции между предикторами).  
   - Проверка: **VIF (Variance Inflation Factor)** > 10 — проблема.  
3. **Гомоскедастичность** остатков (дисперсия ошибок постоянна).  
4. **Нормальность распределения остатков** (для малых выборок).  
5. **Отсутствие автокорреляции** (для временных рядов).  

*Если предположения нарушены, нужны преобразования данных или другие модели (например, гребневая регрессия при мультиколлинеарности).*  

---

## **6. Пример кода на Python**  
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Искусственные данные
np.random.seed(42)
X = pd.DataFrame({
    'x1': np.random.rand(100) * 10,
    'x2': np.random.rand(100) * 5
})
y = 2.5 * X['x1'] + 1.8 * X['x2'] + np.random.normal(0, 1, 100)

# Обучение модели
model = LinearRegression()
model.fit(X, y)

# Результаты
print(f"Коэффициенты: {model.coef_}")  # [2.51, 1.79] ≈ истинным значениям
print(f"R²: {r2_score(y, model.predict(X)):.3f}")  # ~0.95
```

---

## **7. Проблемы и решения**  
| **Проблема**               | **Решение**                          |
|----------------------------|--------------------------------------|
| Мультиколлинеарность       | Удаление признаков, PCA, регуляризация (Lasso/Ridge) |
| Гетероскедастичность       | Преобразование \( y \) (логарифм), WLS |
| Выбросы                    | Robust Regression (например, Theil-Sen) |
| Нелинейность               | Добавление полиномиальных признаков |

---

## **8. Расширения множественной регрессии**  
- **Полиномиальная регрессия:** Добавление \( x^2, x^3 \) для учёта нелинейностей.  
- **Ридж (Ridge) и Лассо (Lasso):** Регуляризация для борьбы с переобучением.  
- **Логистическая регрессия:** Если \( y \) — категориальная переменная.  
